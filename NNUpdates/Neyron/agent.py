# –§–∞–π–ª: NNUpdates/Neyron/agent.py (–û–Ω–æ–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è)

import os
import torch as th
from stable_baselines3 import PPO
from stable_baselines3.common.policies import ActorCriticPolicy
from stable_baselines3.common.callbacks import CheckpointCallback

# –Ü–º–ø–æ—Ä—Ç—É—î–º–æ –Ω–∞—à—ñ –∫–∞—Å—Ç–æ–º–Ω—ñ –∫–ª–∞—Å–∏
from NNUpdates.Neyron.environment import TradingEnv
from NNUpdates.Neyron.model import HybridExtractor

class TradingAgent:
    """
    –ö–µ—Ä—É—î –ø—Ä–æ—Ü–µ—Å–æ–º –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ.
    """
    def __init__(self, env: TradingEnv, log_path: str = "data/NNUpdates/Neyron/logs", model_save_path: str = "data/NNUpdates/Neyron/models"):
        self.env = env
        self.log_path = log_path
        self.model_save_path = model_save_path
        
        os.makedirs(self.log_path, exist_ok=True)
        os.makedirs(self.model_save_path, exist_ok=True)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –ø–æ–ª—ñ—Ç–∏–∫–∏, –¥–µ –≤–∫–∞–∑—É—î–º–æ –Ω–∞—à –∫–∞—Å—Ç–æ–º–Ω–∏–π extractor
        policy_kwargs = dict(
            features_extractor_class=HybridExtractor,
            features_extractor_kwargs=dict(features_dim=128), # –†–æ–∑–º—ñ—Ä –≤–∏—Ö–æ–¥—É –Ω–∞—à–æ–≥–æ LSTM
            net_arch=dict(pi=[64, 64], vf=[64, 64]) # –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è Actor (pi) —ñ Critic (vf) –≥–æ–ª—ñ–≤
        )
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è PPO –º–æ–¥–µ–ª—ñ
        self.model = PPO(
            "MlpPolicy", # –ú–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ MlpPolicy, –∞–ª–µ –∑ –∫–∞—Å—Ç–æ–º–Ω–∏–º extractor'–æ–º
            self.env,
            policy_kwargs=policy_kwargs,
            verbose=1,
            tensorboard_log=self.log_path,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            ent_coef=0.01,
            learning_rate=3e-4,
            device="cuda" if th.cuda.is_available() else "cpu"
        )
        print(f"ü§ñ –ê–≥–µ–Ω—Ç PPO —Å—Ç–≤–æ—Ä–µ–Ω–æ. –ù–∞–≤—á–∞–Ω–Ω—è –±—É–¥–µ –Ω–∞: {self.model.device}")

    def train(self, total_timesteps: int = 100000):
        """–ó–∞–ø—É—Å–∫–∞—î –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è."""
        print(f"\nüî• –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {total_timesteps} –∫—Ä–æ–∫—ñ–≤...")
        
        checkpoint_callback = CheckpointCallback(
            save_freq=10000,
            save_path=self.model_save_path,
            name_prefix="trading_model"
        )
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=True
        )
        
        final_model_path = os.path.join(self.model_save_path, "final_trading_model.zip")
        self.model.save(final_model_path)
        print(f"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {final_model_path}")

    def load_model(self, path: str):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å."""
        self.model = PPO.load(path, env=self.env)
        print(f"üß† –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑: {path}")

    def run_inference(self, episodes: int = 1):
        """–ó–∞–ø—É—Å–∫–∞—î –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó."""
        print("\nüé¨ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ...")
        for ep in range(episodes):
            obs, info = self.env.reset()
            done = False
            total_reward = 0
            while not done:
                action, _states = self.model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = self.env.step(action)
                total_reward += reward
            print(f"   - –ï–ø—ñ–∑–æ–¥ {ep+1}: –§—ñ–Ω–∞–ª—å–Ω–∞ –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ (Sharpe Ratio) = {total_reward:.4f}")